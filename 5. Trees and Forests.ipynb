{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "list.of.packages = c('rpart','caret','ggplot2','randomForest',\n",
    "                     'tidyverse','ranger','e1071','rpart.plot')\n",
    "new.packages = list.of.packages[!(list.of.packages \n",
    "                      %in% installed.packages()[,\"Package\"])]\n",
    "if(length(new.packages)) install.packages(new.packages,\n",
    "                        repos='http://cran.us.r-project.org')\n",
    "\n",
    "options(warn=-1)\n",
    "suppressWarnings(library(rpart, quietly = TRUE, warn.conflicts = FALSE))\n",
    "suppressWarnings(library(rpart.plot, quietly = TRUE, warn.conflicts = FALSE))\n",
    "suppressWarnings(library(ggplot2, quietly = TRUE, warn.conflicts = FALSE))\n",
    "suppressWarnings(library(tidyverse, quietly = TRUE, warn.conflicts = FALSE))\n",
    "suppressWarnings(library(ranger, quietly = TRUE, warn.conflicts = FALSE))\n",
    "suppressWarnings(library(e1071, quietly = TRUE, warn.conflicts = FALSE))\n",
    "suppressWarnings(library(caret, quietly = TRUE, warn.conflicts = FALSE))\n",
    "suppressWarnings(library(randomForest, quietly = TRUE, warn.conflicts = FALSE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module we will re-examine the Housing data. \n",
    "\n",
    "However, instead of assuming the model is linear ($Y = X\\beta + \\varepsilon$) we will be agnostic about functional form.\n",
    "\n",
    "Let $Y=g(X)+\\varepsilon$. To estimate the unknown function $g(X)$, we will use Regression Trees and Forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimesions:  51808 212"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>LOGVALUE</th><th scope=col>LOT</th><th scope=col>UNITSF</th><th scope=col>CLIMB</th><th scope=col>DIRAC</th><th scope=col>NUMAIR</th><th scope=col>BUSPER</th><th scope=col>EXCLUS</th><th scope=col>HOWH</th><th scope=col>NUMCOLD</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>13.48701 </td><td> 5900.00 </td><td>3000     </td><td>2.011952 </td><td>1.51928  </td><td>1.730885 </td><td>0        </td><td>0        </td><td> 8.000000</td><td>1.828194 </td></tr>\n",
       "\t<tr><td>13.21767 </td><td>43925.93 </td><td>1600     </td><td>2.011952 </td><td>1.51928  </td><td>1.730885 </td><td>0        </td><td>0        </td><td> 9.000000</td><td>1.828194 </td></tr>\n",
       "\t<tr><td>13.48701 </td><td> 5900.00 </td><td>2500     </td><td>2.011952 </td><td>1.51928  </td><td>1.730885 </td><td>0        </td><td>0        </td><td> 8.555945</td><td>1.828194 </td></tr>\n",
       "\t<tr><td>13.01700 </td><td> 2000.00 </td><td>1600     </td><td>2.011952 </td><td>1.51928  </td><td>1.730885 </td><td>1        </td><td>0        </td><td> 9.000000</td><td>1.828194 </td></tr>\n",
       "\t<tr><td>13.45884 </td><td> 6000.00 </td><td>2750     </td><td>2.011952 </td><td>1.51928  </td><td>1.730885 </td><td>0        </td><td>0        </td><td>10.000000</td><td>1.828194 </td></tr>\n",
       "\t<tr><td>13.51441 </td><td> 5500.00 </td><td>2350     </td><td>2.011952 </td><td>0.00000  </td><td>1.730885 </td><td>0        </td><td>1        </td><td>10.000000</td><td>1.828194 </td></tr>\n",
       "\t<tr><td>13.21767 </td><td> 6000.00 </td><td>1901     </td><td>2.011952 </td><td>1.51928  </td><td>1.730885 </td><td>0        </td><td>0        </td><td>10.000000</td><td>1.828194 </td></tr>\n",
       "\t<tr><td>14.07787 </td><td> 6500.00 </td><td>5700     </td><td>2.011952 </td><td>1.51928  </td><td>1.730885 </td><td>0        </td><td>0        </td><td>10.000000</td><td>1.828194 </td></tr>\n",
       "\t<tr><td>14.22098 </td><td>10000.00 </td><td>5700     </td><td>2.011952 </td><td>1.00000  </td><td>1.730885 </td><td>0        </td><td>1        </td><td>10.000000</td><td>1.828194 </td></tr>\n",
       "\t<tr><td>11.54248 </td><td> 9000.00 </td><td>2300     </td><td>2.011952 </td><td>1.51928  </td><td>1.730885 </td><td>0        </td><td>0        </td><td>10.000000</td><td>1.828194 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllll}\n",
       " LOGVALUE & LOT & UNITSF & CLIMB & DIRAC & NUMAIR & BUSPER & EXCLUS & HOWH & NUMCOLD\\\\\n",
       "\\hline\n",
       "\t 13.48701  &  5900.00  & 3000      & 2.011952  & 1.51928   & 1.730885  & 0         & 0         &  8.000000 & 1.828194 \\\\\n",
       "\t 13.21767  & 43925.93  & 1600      & 2.011952  & 1.51928   & 1.730885  & 0         & 0         &  9.000000 & 1.828194 \\\\\n",
       "\t 13.48701  &  5900.00  & 2500      & 2.011952  & 1.51928   & 1.730885  & 0         & 0         &  8.555945 & 1.828194 \\\\\n",
       "\t 13.01700  &  2000.00  & 1600      & 2.011952  & 1.51928   & 1.730885  & 1         & 0         &  9.000000 & 1.828194 \\\\\n",
       "\t 13.45884  &  6000.00  & 2750      & 2.011952  & 1.51928   & 1.730885  & 0         & 0         & 10.000000 & 1.828194 \\\\\n",
       "\t 13.51441  &  5500.00  & 2350      & 2.011952  & 0.00000   & 1.730885  & 0         & 1         & 10.000000 & 1.828194 \\\\\n",
       "\t 13.21767  &  6000.00  & 1901      & 2.011952  & 1.51928   & 1.730885  & 0         & 0         & 10.000000 & 1.828194 \\\\\n",
       "\t 14.07787  &  6500.00  & 5700      & 2.011952  & 1.51928   & 1.730885  & 0         & 0         & 10.000000 & 1.828194 \\\\\n",
       "\t 14.22098  & 10000.00  & 5700      & 2.011952  & 1.00000   & 1.730885  & 0         & 1         & 10.000000 & 1.828194 \\\\\n",
       "\t 11.54248  &  9000.00  & 2300      & 2.011952  & 1.51928   & 1.730885  & 0         & 0         & 10.000000 & 1.828194 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "LOGVALUE | LOT | UNITSF | CLIMB | DIRAC | NUMAIR | BUSPER | EXCLUS | HOWH | NUMCOLD | \n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "| 13.48701  |  5900.00  | 3000      | 2.011952  | 1.51928   | 1.730885  | 0         | 0         |  8.000000 | 1.828194  | \n",
       "| 13.21767  | 43925.93  | 1600      | 2.011952  | 1.51928   | 1.730885  | 0         | 0         |  9.000000 | 1.828194  | \n",
       "| 13.48701  |  5900.00  | 2500      | 2.011952  | 1.51928   | 1.730885  | 0         | 0         |  8.555945 | 1.828194  | \n",
       "| 13.01700  |  2000.00  | 1600      | 2.011952  | 1.51928   | 1.730885  | 1         | 0         |  9.000000 | 1.828194  | \n",
       "| 13.45884  |  6000.00  | 2750      | 2.011952  | 1.51928   | 1.730885  | 0         | 0         | 10.000000 | 1.828194  | \n",
       "| 13.51441  |  5500.00  | 2350      | 2.011952  | 0.00000   | 1.730885  | 0         | 1         | 10.000000 | 1.828194  | \n",
       "| 13.21767  |  6000.00  | 1901      | 2.011952  | 1.51928   | 1.730885  | 0         | 0         | 10.000000 | 1.828194  | \n",
       "| 14.07787  |  6500.00  | 5700      | 2.011952  | 1.51928   | 1.730885  | 0         | 0         | 10.000000 | 1.828194  | \n",
       "| 14.22098  | 10000.00  | 5700      | 2.011952  | 1.00000   | 1.730885  | 0         | 1         | 10.000000 | 1.828194  | \n",
       "| 11.54248  |  9000.00  | 2300      | 2.011952  | 1.51928   | 1.730885  | 0         | 0         | 10.000000 | 1.828194  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   LOGVALUE LOT      UNITSF CLIMB    DIRAC   NUMAIR   BUSPER EXCLUS HOWH     \n",
       "1  13.48701  5900.00 3000   2.011952 1.51928 1.730885 0      0       8.000000\n",
       "2  13.21767 43925.93 1600   2.011952 1.51928 1.730885 0      0       9.000000\n",
       "3  13.48701  5900.00 2500   2.011952 1.51928 1.730885 0      0       8.555945\n",
       "4  13.01700  2000.00 1600   2.011952 1.51928 1.730885 1      0       9.000000\n",
       "5  13.45884  6000.00 2750   2.011952 1.51928 1.730885 0      0      10.000000\n",
       "6  13.51441  5500.00 2350   2.011952 0.00000 1.730885 0      1      10.000000\n",
       "7  13.21767  6000.00 1901   2.011952 1.51928 1.730885 0      0      10.000000\n",
       "8  14.07787  6500.00 5700   2.011952 1.51928 1.730885 0      0      10.000000\n",
       "9  14.22098 10000.00 5700   2.011952 1.00000 1.730885 0      1      10.000000\n",
       "10 11.54248  9000.00 2300   2.011952 1.51928 1.730885 0      0      10.000000\n",
       "   NUMCOLD \n",
       "1  1.828194\n",
       "2  1.828194\n",
       "3  1.828194\n",
       "4  1.828194\n",
       "5  1.828194\n",
       "6  1.828194\n",
       "7  1.828194\n",
       "8  1.828194\n",
       "9  1.828194\n",
       "10 1.828194"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Housing = read.csv('ahs_clean.csv')   \n",
    "# Reads csv files into R\n",
    "\n",
    "cat('Dataset dimesions: ',dim(Housing))                  \n",
    "# Returns the dimension of the dataset             \n",
    "\n",
    "Housing$LOGSQFT = log(Housing$UNITSF)\n",
    "Housing$LOGLOT = log(Housing$LOT)\n",
    "\n",
    "Housing[1:10,1:10]                                         \n",
    "# Examine the first few rows and columns of our data                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we already examined this data, let's go ahead and use the larger model with 60 covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data\n",
    "\n",
    "data_split = function(data,split=.1){\n",
    "\n",
    "    N = dim(data)[1]                         \n",
    "    random = sample(1:N,N,replace=F)          \n",
    "    # Randomly choose the index for the splits\n",
    "    data = data[random,]                      \n",
    "    # Shuffle data\n",
    "    test = data[1:round(N*split),]            \n",
    "    # Splits\n",
    "    train = data[(round(N*split)+1):N,]\n",
    "\n",
    "    \n",
    "    return(list(train,test))\n",
    "}\n",
    "\n",
    "\n",
    "tmp = data_split(Housing) \n",
    "train_test = tmp[[1]]\n",
    "validation = tmp[[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train1 = train_test[c('LOGVALUE','LOGLOT','LOGSQFT','BEDRMS','BATHS',\n",
    "                      'REGION','METRO','KITCHEN','DISH','WASH','DRY',\n",
    "                      'COOK','DENS','DINING','FAMRM','HALFB','LIVING',\n",
    "                      'OTHFN','RECRM','PORCH','SINK','LAUNDY','FLOORS',\n",
    "                      'CONDO','ROOMS','PLUMB','NOWIRE','AGE')]\n",
    "\n",
    "train1 = cbind(train1,select(train_test,starts_with('Roach')),\n",
    "               select(train_test,starts_with('Rats')),\n",
    "              select(train_test,starts_with('Water')),\n",
    "               select(train_test,starts_with('Cellar')),\n",
    "               select(train_test,starts_with('Num')),\n",
    "               select(train_test,starts_with('Freeze')))\n",
    "\n",
    "\n",
    "val1 = validation[colnames(train1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the MSE case the regression tree will predict: \n",
    "\n",
    "$\\hat{c}_m = \\frac{\\sum_i y_i 1(x\\in R_m)}{\\sum_i 1(x\\in R_m)}$, which is nothing more than the average outcome given the covariate values in the region.\n",
    "\n",
    "\n",
    "When fitting a regression tree one needs to decide how to generate the splits that make up the tree. Further one needs to decide how deep to make the tree. There are different data-driven approaches to making these selections. In this module we will choose the model based on the complexity:\n",
    "\n",
    "$C_\\alpha(T) = \\sum_m N_m Q_m(T) + \\alpha |T|$\n",
    "\n",
    "$N_m$ is the number of observations in node $m$\n",
    "\n",
    "$|T|$ is the number of terminal nodes in tree T\n",
    "\n",
    "$Q_m(T)=\\frac{1}{N_m} \\sum_{i:x_i\\in R_m} (y_i-\\hat{c}_m)^2$\n",
    "\n",
    "Instead of manually choosing the depth of our tree, we will use Carat to tune the penalty $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>cp</th><th scope=col>RMSE</th><th scope=col>Rsquared</th><th scope=col>MAE</th><th scope=col>RMSESD</th><th scope=col>RsquaredSD</th><th scope=col>MAESD</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.03183627 </td><td>0.9741609  </td><td>0.17336979 </td><td>0.6143909  </td><td>0.01742652 </td><td>0.024558019</td><td>0.008451061</td></tr>\n",
       "\t<tr><td>0.06597948 </td><td>0.9986146  </td><td>0.13124228 </td><td>0.6381598  </td><td>0.02226196 </td><td>0.033147736</td><td>0.026769000</td></tr>\n",
       "\t<tr><td>0.09747511 </td><td>1.0414115  </td><td>0.09152347 </td><td>0.6792751  </td><td>0.03000983 </td><td>0.002061573</td><td>0.031519464</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllll}\n",
       " cp & RMSE & Rsquared & MAE & RMSESD & RsquaredSD & MAESD\\\\\n",
       "\\hline\n",
       "\t 0.03183627  & 0.9741609   & 0.17336979  & 0.6143909   & 0.01742652  & 0.024558019 & 0.008451061\\\\\n",
       "\t 0.06597948  & 0.9986146   & 0.13124228  & 0.6381598   & 0.02226196  & 0.033147736 & 0.026769000\\\\\n",
       "\t 0.09747511  & 1.0414115   & 0.09152347  & 0.6792751   & 0.03000983  & 0.002061573 & 0.031519464\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "cp | RMSE | Rsquared | MAE | RMSESD | RsquaredSD | MAESD | \n",
       "|---|---|---|\n",
       "| 0.03183627  | 0.9741609   | 0.17336979  | 0.6143909   | 0.01742652  | 0.024558019 | 0.008451061 | \n",
       "| 0.06597948  | 0.9986146   | 0.13124228  | 0.6381598   | 0.02226196  | 0.033147736 | 0.026769000 | \n",
       "| 0.09747511  | 1.0414115   | 0.09152347  | 0.6792751   | 0.03000983  | 0.002061573 | 0.031519464 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  cp         RMSE      Rsquared   MAE       RMSESD     RsquaredSD  MAESD      \n",
       "1 0.03183627 0.9741609 0.17336979 0.6143909 0.01742652 0.024558019 0.008451061\n",
       "2 0.06597948 0.9986146 0.13124228 0.6381598 0.02226196 0.033147736 0.026769000\n",
       "3 0.09747511 1.0414115 0.09152347 0.6792751 0.03000983 0.002061573 0.031519464"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model <- train(\n",
    "  LOGVALUE ~.,data = train1,               \n",
    "    # formula for the model\n",
    "  method = \"rpart\",                           \n",
    "    # Regression Trees\n",
    "  trControl = trainControl(\n",
    "    method = \"cv\", \n",
    "    number = 5,             \n",
    "      # 5 fold cross-validatoin\n",
    "  )\n",
    ")\n",
    "\n",
    "model$results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The penalty parameter $\\alpha$ chosen in this case is $\\approx 0.032$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = as.formula(paste(\"LOGVALUE~\",\n",
    "            paste(colnames(train1)[1:dim(train1)[2]],collapse=\"+\")))\n",
    "\n",
    "fit_tree = rpart(formula,data=train1, control=c(cp=model$bestTune))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize and interpret the regression tree with the following plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAABv1BMVEUAAAAgNEEoOUIrR1cw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\nM7ZHBPAJxQAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rpart.plot(fit_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see predicted (rounded) values for this tree above the frequency of those predictions. Let's see how we do on the validation set with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tree RMSE:  1.086604"
     ]
    }
   ],
   "source": [
    "yhat = predict(fit_tree,val1)\n",
    "rmse = sqrt(mean((val1$LOGVALUE-yhat)^2))\n",
    "cat('\\nTree RMSE: ', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trees are great for interpretation, but not the best for actual prediction. The RMSE we got is worse than the linear models we estimated in the previous module.\n",
    "\n",
    "One idea that is common in machine learning is to combine weak learners together and produce an ensemble. When one does this with trees the method is known as random forests.\n",
    "\n",
    "The basic idea is combine many shallow trees with only a subset of the covariates and average the predictions. Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid = expand.grid(mtry = c(5, 10, 15), \n",
    "          splitrule = c(\"variance\"), min.node.size=c(5))\n",
    "fit_forest = train(formula,\n",
    "                   data = train1, \n",
    "                   method = 'ranger',\n",
    "                   trControl = \n",
    "                   trainControl(method = 'cv',\n",
    "                                number = 5),\n",
    "                   tuneGrid = rf_grid, \n",
    "                   num.trees=10\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>mtry</th><th scope=col>splitrule</th><th scope=col>min.node.size</th><th scope=col>RMSE</th><th scope=col>Rsquared</th><th scope=col>MAE</th><th scope=col>RMSESD</th><th scope=col>RsquaredSD</th><th scope=col>MAESD</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td> 5         </td><td>variance   </td><td>5          </td><td>0.8416696  </td><td>0.3840645  </td><td>0.5010820  </td><td>0.03172367 </td><td>0.01575172 </td><td>0.007301056</td></tr>\n",
       "\t<tr><td>10         </td><td>variance   </td><td>5          </td><td>0.8429754  </td><td>0.3818747  </td><td>0.4989303  </td><td>0.02899679 </td><td>0.01553277 </td><td>0.006370934</td></tr>\n",
       "\t<tr><td>15         </td><td>variance   </td><td>5          </td><td>0.8431393  </td><td>0.3833763  </td><td>0.5003073  </td><td>0.03158785 </td><td>0.02073955 </td><td>0.007543335</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllll}\n",
       " mtry & splitrule & min.node.size & RMSE & Rsquared & MAE & RMSESD & RsquaredSD & MAESD\\\\\n",
       "\\hline\n",
       "\t  5          & variance    & 5           & 0.8416696   & 0.3840645   & 0.5010820   & 0.03172367  & 0.01575172  & 0.007301056\\\\\n",
       "\t 10          & variance    & 5           & 0.8429754   & 0.3818747   & 0.4989303   & 0.02899679  & 0.01553277  & 0.006370934\\\\\n",
       "\t 15          & variance    & 5           & 0.8431393   & 0.3833763   & 0.5003073   & 0.03158785  & 0.02073955  & 0.007543335\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "mtry | splitrule | min.node.size | RMSE | Rsquared | MAE | RMSESD | RsquaredSD | MAESD | \n",
       "|---|---|---|\n",
       "|  5          | variance    | 5           | 0.8416696   | 0.3840645   | 0.5010820   | 0.03172367  | 0.01575172  | 0.007301056 | \n",
       "| 10          | variance    | 5           | 0.8429754   | 0.3818747   | 0.4989303   | 0.02899679  | 0.01553277  | 0.006370934 | \n",
       "| 15          | variance    | 5           | 0.8431393   | 0.3833763   | 0.5003073   | 0.03158785  | 0.02073955  | 0.007543335 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  mtry splitrule min.node.size RMSE      Rsquared  MAE       RMSESD    \n",
       "1  5   variance  5             0.8416696 0.3840645 0.5010820 0.03172367\n",
       "2 10   variance  5             0.8429754 0.3818747 0.4989303 0.02899679\n",
       "3 15   variance  5             0.8431393 0.3833763 0.5003073 0.03158785\n",
       "  RsquaredSD MAESD      \n",
       "1 0.01575172 0.007301056\n",
       "2 0.01553277 0.006370934\n",
       "3 0.02073955 0.007543335"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit_forest$results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with only 10 trees the results are improved!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCISE:\n",
    "\n",
    "Grow forests with 100 trees and use caret to tune the maximum number of covariates. Do your results improve?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
